{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dataset is called MNIST and refers to handwritten digit recognition.\n",
    "### You can find more about it on Yann LeCun's website (Director of AI Research, Facebook).\n",
    "### The goal is to write an algorithm that detects which digit is written. Since there are only 10 digits (0, 1, 2, 3, 4, 5, 6, 7, 8, 9), this is a classification problem with 10 classes.\n",
    "\n",
    "### Our goal would be to build a neural network with 2 hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the packages\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import collections\n",
    "\n",
    "# visualization tools\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading dataset\n",
    "mnist_dataset, mnist_info = tfds.load(name='mnist' ,with_info=True,as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'PrefetchDataset' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-6886d7f6802b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mnum_test_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_test_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmnist_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmnist_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'PrefetchDataset' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# extracting training and test dataset\n",
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test']\n",
    "\n",
    "# validation dataset setup\n",
    "num_validation_samples = 0.1*mnist_info.splits['train'].num_examples\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)\n",
    "\n",
    "print(mnist_train[0])\n",
    "\n",
    "plt.imshow(mnist_train[0, :, :, 0])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling data by function\n",
    "def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255.\n",
    "    return image, label\n",
    "\n",
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "scaled_test_and_validation_data = mnist_test.map(scale)\n",
    "\n",
    "# suffling dataset to properly apply sgd\n",
    "Buffer_size = 10000\n",
    "\n",
    "shuffled_train_validation_data = scaled_train_and_validation_data.shuffle(Buffer_size)\n",
    "\n",
    "# preparing data in validation and train format\n",
    "validation_data = shuffled_train_validation_data.take(num_validation_samples)\n",
    "\n",
    "trained_data = shuffled_train_validation_data.skip(num_validation_samples)\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "trained_data = trained_data.batch(batch_size)\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "test_data = scaled_test_and_validation_data.batch(num_test_samples)\n",
    "\n",
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "# Use same hidden layer size for both hidden layers. Not a necessity.\n",
    "hidden_layer_size = 100\n",
    "    \n",
    "# define how the model will look like\n",
    "model = tf.keras.Sequential([\n",
    "    \n",
    "    # the first layer (the input layer)\n",
    "    # each observation is 28x28x1 pixels, therefore it is a tensor of rank 3\n",
    "    # since we don't know CNNs yet, we don't know how to feed such input into our net, so we must flatten the images\n",
    "    # there is a convenient method 'Flatten' that simply takes our 28x28x1 tensor and orders it into a (None,) \n",
    "    # or (28x28x1,) = (784,) vector\n",
    "    # this allows us to actually create a feed forward neural network\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28, 1)), # input layer\n",
    "    \n",
    "    # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)\n",
    "    # it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 3rd hidden layer    \n",
    "    # the final layer is no different, we just make sure to activate it with softmax\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "540/540 - 9s - loss: 0.3319 - accuracy: 0.9038 - val_loss: 0.1487 - val_accuracy: 0.9575\n",
      "Epoch 2/10\n",
      "540/540 - 7s - loss: 0.1327 - accuracy: 0.9605 - val_loss: 0.1168 - val_accuracy: 0.9657\n",
      "Epoch 3/10\n",
      "540/540 - 8s - loss: 0.0937 - accuracy: 0.9716 - val_loss: 0.0901 - val_accuracy: 0.9747\n",
      "Epoch 4/10\n",
      "540/540 - 8s - loss: 0.0731 - accuracy: 0.9774 - val_loss: 0.0682 - val_accuracy: 0.9803\n",
      "Epoch 5/10\n",
      "540/540 - 8s - loss: 0.0583 - accuracy: 0.9819 - val_loss: 0.0613 - val_accuracy: 0.9798\n",
      "Epoch 6/10\n",
      "540/540 - 8s - loss: 0.0458 - accuracy: 0.9862 - val_loss: 0.0549 - val_accuracy: 0.9820\n",
      "Epoch 7/10\n",
      "540/540 - 8s - loss: 0.0399 - accuracy: 0.9874 - val_loss: 0.0409 - val_accuracy: 0.9875\n",
      "Epoch 8/10\n",
      "540/540 - 8s - loss: 0.0329 - accuracy: 0.9894 - val_loss: 0.0467 - val_accuracy: 0.9853\n",
      "Epoch 9/10\n",
      "540/540 - 7s - loss: 0.0341 - accuracy: 0.9890 - val_loss: 0.0387 - val_accuracy: 0.9878\n",
      "Epoch 10/10\n",
      "540/540 - 7s - loss: 0.0249 - accuracy: 0.9922 - val_loss: 0.0414 - val_accuracy: 0.9882\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14d2345e408>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#custom_optimizer = tf.keras.optimizers.SGD(learning_rate=0.02)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#training the model \n",
    "num_epochs = 10\n",
    "model.fit(trained_data, epochs = num_epochs, validation_data=(validation_inputs, validation_targets), verbose = 2)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
